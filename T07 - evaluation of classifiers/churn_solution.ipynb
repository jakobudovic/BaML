{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3498125e-3f8d-46c7-805d-8b31237a8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2169d-b97f-458e-8529-8281f676212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_excel('e-commerce-dataset.xlsx', sheet_name='E_Comm')\n",
    "df.to_csv('e-commerce_churn.csv', index=False)\n",
    "df = pd.read_csv('e-commerce_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f2b0d-c5a6-48aa-8de1-2b1e2f5dcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedab030-5e09-48d4-a29e-d9cf8633036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'] = df['Churn'].astype('category')\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply one-hot encoding to non-numeric columns\n",
    "df = pd.get_dummies(df, columns=non_numeric_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7baf72-11a3-4b81-aa35-b3be45e22165",
   "metadata": {},
   "source": [
    "## Exercise T8.3 a)\n",
    "\n",
    "Your colleague proposes to train the model on the entire dataset and argues to tune the n_estimators\n",
    "and max_features parameters of sklearn.ensemble.RandomForestClassifier until the training\n",
    "accuracy is maximized. Do you agree? If not, which issues can you identify with this approach?\n",
    "\n",
    "What is the accuracy of your random forest model on the training dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97d19c-dcfb-49f6-b331-0de391480ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest model\n",
    "train_model = RandomForestClassifier(n_estimators=5, max_features=3, random_state=2023+2024)\n",
    "\n",
    "X = df.drop(columns=[\"Churn\"])\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "train_model.fit(X,y)\n",
    "\n",
    "# Calculate accuracy\n",
    "pred = train_model.predict(X)\n",
    "error_rate = np.mean(y != pred)\n",
    "print(\"Error rate:\", error_rate)\n",
    "print(\"Accuracy:\", accuracy_score(y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb4be0-6358-4367-a66b-328456b17b1e",
   "metadata": {},
   "source": [
    "## Exercise T8.3 c)\n",
    "\n",
    "Perform training, 4-fold cross-validation, and testing with a 60-20-20 % split in Python. Use the\n",
    "precision as metric for model selection. Build a confusion matrix for the test set and report precision,\n",
    "accuracy, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87a5e5-71ba-403b-b170-3a58239048d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.20, stratify=df['Churn'], random_state=2023+2024)\n",
    "\n",
    "# Check label balancing\n",
    "print(\"Label Balancing in Train Set:\\n\", train_df['Churn'].value_counts(normalize=True))\n",
    "print(\"Label Balancing in Test Set:\\n\", test_df['Churn'].value_counts(normalize=True))\n",
    "\n",
    "# Define some Models\n",
    "train_model_1 = RandomForestClassifier(n_estimators=5,\n",
    "                                       max_features=3,\n",
    "                                       random_state=2023+2024)\n",
    "\n",
    "train_model_2 = RandomForestClassifier(n_estimators=8,\n",
    "                                       max_features=3,\n",
    "                                       random_state=2023+2024)\n",
    "\n",
    "train_model_3 = RandomForestClassifier(n_estimators=5,\n",
    "                                       max_features=10,\n",
    "                                       random_state=2023+2024)\n",
    "\n",
    "# Split features and labels\n",
    "X = train_df.drop(columns=[\"Churn\"])\n",
    "y = train_df[\"Churn\"]\n",
    "\n",
    "# Perform cross-validation on all three models and choose the one with the highest\n",
    "#  average precision (across all folds)\n",
    "best_model = None\n",
    "best_score = -1\n",
    "for i, train_model in enumerate([train_model_1, train_model_2, train_model_3]):\n",
    "  print(\"train_model_\"+str(i+1)+\":\")\n",
    "  score = np.mean(cross_val_score(train_model, X, y, cv=4, scoring=\"precision\"))\n",
    "  print(\"Average score across all folds:\", score)\n",
    "  if score >= best_score:\n",
    "    best_score = score\n",
    "    best_model = train_model\n",
    "\n",
    "\n",
    "# Train the final model (no cross-validation)\n",
    "print(\"\\nbest_model:\", best_model.n_estimators, best_model.max_features)\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Report scores on final model\n",
    "y_pred = best_model.predict(X)\n",
    "print(\"\\nPrecision:\", precision_score(y, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"Recall:\", recall_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df54d6-07db-4cea-ae38-18bce02d9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Importance Plot\n",
    "importance_values = best_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance_values})\n",
    "imp_plot = importance_df.plot(kind='bar', x='Feature', y='Importance', legend=False)\n",
    "imp_plot.plot()\n",
    "plt.show()\n",
    "\n",
    "# Apply on test set\n",
    "test_predictions = best_model.predict(test_df.drop(columns=['Churn']))\n",
    "test_probabilities = best_model.predict_proba(test_df.drop(columns=['Churn']))\n",
    "\n",
    "test_predictions_df = pd.DataFrame({'Churn': test_df['Churn'], \n",
    "                                     'Predicted_Churn': test_predictions,\n",
    "                                     'Probability_Churn=0': test_probabilities[:, 0],\n",
    "                                     'Probability_Churn=1': test_probabilities[:, 1]})\n",
    "\n",
    "print(test_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0c8e9-971b-49b1-bc17-21320c56d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(test_df['Churn'], test_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Precision, accuracy, recall\n",
    "print(\"\\nTest-Precision:\", precision_score(test_df['Churn'], test_predictions))\n",
    "print(\"Test-Accuracy:\", accuracy_score(test_df['Churn'], test_predictions))\n",
    "print(\"Test-Recall:\", recall_score(test_df['Churn'], test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd09b5-cfcd-4921-baa3-9c857d18ceed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
